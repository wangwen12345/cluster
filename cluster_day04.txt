feature 特征    网站:http://docs.ceph.org.cn/start/intro/
元数据 描述数据属性的信息
分布式文件系统dfs的定义(distributed file system)
  文件系统管理的物理存储资源不一定直接连接在本地节点上,而是通过计算机网络与节点相连
  分布式文件系统的设计基本客户机/服务器模式
特点:高扩展型,高可用,高性能
常见的分布式文件系统
    Lustre
     hadoop(大数据)
     fastdfs
      ceph (红帽收购了)
        clusterfs

ceph定义
  是一个分布式文件系统
  具有高扩展 高可用 高性能的特点
  可以提供对象存储 块存储 文件系统存储
  可以提供PB级别的存储空间
  软件定义存储(software defined storage)

ceph组件
   OSDs     存储设备(object storage device)
   Monitors 集群监控 组件
   MDSs     存放文件系统的元数据(对象存储和块存储不需要该组件)
   client   ceph客户端

254:
yum  -y   install  vsftpd
mkdir /var/ftp/rhel
mkdir /var/ftp/ceph
mount  -o  loop  目录名/rhel-server-7.4-x86_64-dvd.iso /var/ftp/rhel
mount  -o loop 目录名/ceph/rhcs2.0-rhosp9-20161113-x86_64.iso  /var/ftp/ceph
]# systemctl  restart  vsftpd.service
++++++++++++++++++++++++++
50-55:
]# cat /etc/yum.repos.d/plj.repo 
[rhel]
name=rhel
baseurl=ftp://192.168.4.254/rhel
enabled=1
gpgcheck=0
[mon]
name=cephmon
baseurl=ftp://192.168.4.254/ceph/rhceph-2.0-rhel-7-x86_64/MON
enabled=1
gpgcheck=0
[osd]
name=cephosd
baseurl=ftp://192.168.4.254/ceph/rhceph-2.0-rhel-7-x86_64/OSD
enabled=1
gpgcheck=0
[tools]
name=cephtools
baseurl=ftp://192.168.4.254/ceph/rhceph-2.0-rhel-7-x86_64/Tools
enabled=1
gpgcheck=0
[root@host51 yum.repos.d]#
  766  yum clean all
  767  yum repolist 
+++++++++++++++++++++++++++++++++++++++++
50-55:
vim /etc/hosts
192.168.4.50	client
192.168.4.51	node1
192.168.4.52	node2
192.168.4.53	node3
192.168.4.54	node4
192.168.4.55	node5
:wq
+++++++++++++++++++++++++
50-55:
  808  rm -rf /root/.ssh

51:
  809  ssh-keygen 
  810  ls /root/.ssh/
  811  ssh-copy-id  root@192.168.4.50
  812  ssh-copy-id  root@192.168.4.51
  813  ssh-copy-id  root@192.168.4.52
  814  ssh-copy-id  root@192.168.4.53
  815  ssh-copy-id  root@192.168.4.54
  816  ssh-copy-id  root@192.168.4.55

+++++++++++++++++++++++++++++++++++++++
50: ntp-server
[root@client ~]# rpm -q chrony
chrony-3.1-2.el7.x86_64

]# vim /etc/chrony.conf 
server 0.centos.pool.ntp.org iburst
allow 192.168.4.0/24
local stratum 10
:wq

21  systemctl  restart chronyd
]#date

51-55
[root@client ~]# rpm -q chrony
chrony-3.1-2.el7.x86_64

]# vim /etc/chrony.conf 
  3 #server 0.rhel.pool.ntp.org iburst
  4 #server 1.rhel.pool.ntp.org iburst
  5 #server 2.rhel.pool.ntp.org iburst
  6 server 192.168.4.50 iburst
21  systemctl  restart chronyd
测试方法： date -s  "2018-09-04 13:10:18"  
           systemctl  restart chronyd
           date

二、创建集群
2.1 创建工作目录
node1~]# mkdir ceph_cluster
node1 ~]# cd ceph_cluster

2.3 安装ceph软件包51
node1 ~]# yum -y  install ceph-deploy
nodes ~]# which ceph-deploy
node1 ~]# ceph-deploy --help

2.4 创建集群
node1~]# ceph-deploy  new  node1  node2  node3

2.5 给集群中的主机安装软件包
node1~]# ceph-deploy  install  node1  node2  node3
 
++++++++++++++++++++++++++++++++++++++++++++++
删除创建的ceph集群:
方法1：在node1  nede2  node3 上执行如下命令
rm  -rf  /var/lib/ceph/*
rm  -rf /var/run/ceph/*
rm  -rf /var/log/ceph/*
rm  -rf /etc/ceph/*

方法2：]# ceph-deploy  uninstall  node1  node2  node3   
+++++++++++++++++++++++++++++++++++++++++++++
2.6 初始化 集群中主机的monitor服务
[root@node1 ceph_cluster]# ceph-deploy mon create-initial

2.7 创建日志盘：在node1 、node2 、 node3 主机上都要执行如下操作 
   79  lsblk 
   80  parted  /dev/vdb  mklabel  gpt
   81  parted  /dev/vdb  mkpart primary 1M 50%
   82  parted  /dev/vdb  mkpart primary 50% 100%
   83  lsblk 
          blkid /dev/vdb     //查看vdb格式是否是gpt
++++++++++++++++++++++++++++++++++++++++
node1  ]#chown  ceph:ceph /dev/vdb*
echo "chown ceph:ceph /dev/vdb*" >> /etc/rc.local
chmod +x /etc/rc.local

node2  ]#chown  ceph:ceph /dev/vdb*
echo "chown ceph:ceph /dev/vdb*" >> /etc/rc.local
chmod +x /etc/rc.local

node3  ]#chown  ceph:ceph /dev/vdb*
echo "chown ceph:ceph /dev/vdb*" >> /etc/rc.local
chmod +x /etc/rc.local

node1  node2  node3 :
[root@node1 ~]# vim /etc/udev/rules.d/50-ceph.rules 
ACTION=="add", KERNEL=="vdb[12]", OWNER="ceph", GROUP="ceph"
:wq

]# vim /etc/rc.local
chown  ceph:ceph  /dev/vdb1
chown  ceph:ceph  /dev/vdb2
systemctl restart ceph\*.service ceph\*.target
:wq


在管理主机node1上初始化数据存储盘：
]#cd /root/ceph_cluster/
]#ceph-deploy disk zap node1:vdc node1:vdd
]#ceph-deploy disk zap node2:vdc node2:vdd
]#ceph-deploy disk zap node3:vdc node3:vdd

创建osd(指定数据存储盘使用的日志盘,在管理主机node1 上执行如下命令即可)
]#ceph-deploy osd create node1:vdc:/dev/vdb1  node1:vdd:/dev/vdb2
//创建osd存储设备，vdc为集群提供存储空间，vdb1提供JOURNAL日志，
//一个存储设备对应一个日志设备，日志需要SSD，不需要很大
]#ceph-deploy osd create node2:vdc:/dev/vdb1  node2:vdd:/dev/vdb2

]#ceph-deploy osd create node3:vdc:/dev/vdb1  node3:vdd:/dev/vdb2
+++++++++++++++++++++++++++++++++++++++++
[root@node1 ceph_cluster]# ceph -s
    cluster ac895181-3849-417b-a523-e455fee21e76
     health HEALTH_OK
     monmap e1: 3 mons at {node1=192.168.4.51:6789/0,node2=192.168.4.52:6789/0,node3=192.168.4.53:6789/0}
            election epoch 6, quorum 0,1,2 node1,node2,node3
     osdmap e38: 6 osds: 6 up, 6 in
            flags sortbitwise
      pgmap v83: 64 pgs, 1 pools, 0 bytes data, 0 objects
            203 MB used, 61170 MB / 61373 MB avail
                  64 active+clean
[root@node1 ceph_cluster]# 

[root@node1 ceph_cluster]# systemctl  status ceph
ceph-create-keys@node1.service  ceph-mon.target                 ceph-osd.target
ceph-mds.target                 ceph-osd@0.service              ceph-radosgw.target
ceph-mon@node1.service          ceph-osd@1.service              ceph.target

+++++++++++++++++++++++++++++++++++++++++++++
ceph块设备也叫RADOS块设备简称RBD(RADOS block device)

块存储：
node1 :
显示已有存储池 ceph osd lspools

创建镜像     rbd create 镜像名 --image-feature  layering --size  数字G
--image-feature 指定RBD映像特征为layering(支持分层)

查看已有镜像 rbd  list

查看已有镜像的信息 rbd info 镜像名称

缩小到镜像的容量  rbd resize --size 数字G 镜像名称  --allow-shrink(允许缩小)

扩大镜像的容量  rbd resize --size 数字G 镜像名称


++++++++++++++++++++++++++++++++++++++++++++++
client:客户端使用镜像存储数据：

查看已有镜像       rbd  list
查看已有镜像的信息 rbd info 镜像名称
查看映射    rbd showmapped
映射        rbd map 镜像名   //将镜像映射为本地磁盘
分区
格式化
挂载
存储数据
卸载
撤销映射 rbd  unmap /dev/rbd/pool_name/image_name
+++++++++++++++++++++++++++++++++++++++++

node1 主机本机是osd设备
[root@node1 ~]# rbd list
demo-image
image
[root@node1 ~]# rbd map image
/dev/rbd0
[root@node1 ~]# mkfs.ext4 /dev/rbd0
[root@node1 ~]# mkdir  /dir2
[root@node1 ~]# mount /dev/rbd1 /dir2
[root@node1 ~]# df -h /dir2
文件系统        容量  已用  可用 已用% 挂载点
/dev/rbd0       5.0G   33M  5.0G    1% /dir2
[root@node1 ~]# echo 123456  > /dir2/a.txt
[root@node1 ~]# cat /dir2/a.txt 
123456
[root@node1 ~]#

]#umount  /dir2
]#rbd unmap /dev/rbd/rbd/image      //取消设备映射
++++++++++++++++++++++++++++++++++++
在远端客户端主机使用ceph集群存储数据
1、在管理主机上创建镜像文件
node1 ~]# rbd create image --image-feature  layering --size 10G
rbd create demo-image --image-feature  layering --size 10G
2、远端客户端主机50使用ceph集群存储数据
装包
]# yum -y  install ceph-common
从管理主机51上拷贝集群配置文件
]# scp 192.168.4.51:/etc/ceph/ceph.conf  /etc/ceph/
]# scp 192.168.4.51:/etc/ceph/ceph.client.admin.keyring /etc/ceph/

]# ls /etc/ceph/
ceph.client.admin.keyring  ceph.conf  rbdmap

client ~]# which rbd
/usr/bin/rbd
[root@client ~]# rbd list
demo-image
image
[root@client ~]# rbd showmapped
[root@client ~]# rbd map image
/dev/rbd0
[root@client ~]# lsblk 
NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0            11:0    1 1024M  0 rom  
vda           252:0    0   20G  0 disk 
├─vda1        252:1    0    1G  0 part /boot
└─vda2        252:2    0   19G  0 part 
  ├─rhel-root 253:0    0   17G  0 lvm  /
  └─rhel-swap 253:1    0    2G  0 lvm  [SWAP]
rbd0          251:0    0    10G  0 disk 
[root@client ~]#mkfs.ext4 /dev/rbd0
[root@client ~]# rbd showmapped
id pool image  snap device    
0  rbd  image5 -    /dev/rbd0
[root@client ~]# mkdir   /rbd0
[root@client ~]# mount /dev/rbd0  /rbd0
[root@client ~]# echo ababab  > /rbd0/test.txt
[root@client ~]# 
[root@client ~]# cat /rbd0/test.txt
ababab

+++++++++++++++++++++++++++++++
创建镜像快照(COW技术 copy online write)
对源文件做写操作时,会被复制到快照文件中

node1]#
查看镜像快照
[root@node1 ~]# rbd list
demo-image
image
[root@node1 ~]#  rbd snap ls image    //查看镜像是否有快照

给已有镜像创建快照
]#rbd snap create image --snap image-snap1

ot@node1 ~]#  rbd snap ls image
SNAPID NAME            SIZE 
     4 image-snap1 10240 MB 

ot@client ~]# rbd  showmapped      
id pool image  snap device    
0  rbd  demo-image -    /dev/rbd0 
[root@client ~]# mount  |  grep  /dev/rbd0
/dev/rbd0 on /rbd0 type ext4 (rw,relatime,seclabel,stripe=1024,data=ordered)
[root@client ~]# ls /rbd0/
lost+found  test.txt
[root@client ~]# cat /rbd0/test.txt 
ababab

client]# rm  -rf  /rbd0/test.txt

__________________________

还原快照 恢复数据
[root@node1 ~]# rbd snap rollback image --snap image-snap1
Rolling back to snapshot: 100% complete...done.
[root@node1 ~]#

client:
cat: /rbd0/test.txt: 没有那个文件或目录
[root@client ~]# umount /rbd0
[root@client ~]# mount /dev/rbd0 /rbd0
[root@client ~]# cat /rbd0/test.txt 
ababab

================快照克隆=====================
克隆快照
[root@node1 ~]# rbd snap protect image --snap image-snap1  //快照写操作保护
[root@node1 ~]# rbd snap rm image --snap image-snap1       //会失败
[root@node1 ~]# rbd clone  image --snap image-snap1 image-clone --image-feature layering
[root@node1 ~]# rbd snap unprotect image --snap image-snap1   //快照取消保护

//使用image的快照image-snap1克隆一个新的image-clone镜像
[root@node1 ~]# rbd info image-clone                         //查看克隆快照信息
rbd image 'image-clone':
	size 10240 MB in 2560 objects
	order 22 (4096 kB objects)
	block_name_prefix: rbd_data.5e53238e1f29
	format: 2
	features: layering
	flags: 
	parent: rbd/image@image-snap1
	overlap: 10240 MB
[root@node1 ~]#  rbd flatten image-clone   //还原镜像image
[root@node1 ~]#  rbd info image-clone
rbd image 'image-clone':
    size 15360 MB in 3840 objects
    order 22 (4096 kB objects)
    block_name_prefix: rbd_data.d3f53d1b58ba
    format: 2
    features: layering
    flags: 
#注意，父快照信息没了！

步骤四：其他操作

1） 客户端撤销磁盘映射
[root@client ~]# umount /mnt
[root@client ~]# rbd showmapped
id pool image        snap device    
0  rbd  image        -    /dev/rbd0

[root@client ~]# rbd unmap /dev/rbd/{poolname}/{imagename}    //语法格式
[root@client ~]# rbd unmap /dev/rbd/rbd/image
2）删除快照与镜像
[root@node1 ~]# rbd snap rm image --snap image-snap1   //删除快照
[root@node1 ~]# rbd  list 
[root@node1 ~]# rbd  rm  image                         //删除镜像














































